{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918c9ff6-c773-458d-851a-6cb8c970dc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.data_utils.utils import InputExample\n",
    "import os\n",
    "import json, csv\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "from openprompt.utils.logging import logger\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579f2db3-a925-44f2-8e20-bc0ff4bb8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraChatProcessor(DataProcessor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.labels = None\n",
    "\n",
    "    def get_examples(self, data_path: str) -> List[InputExample]:\n",
    "        examples = []\n",
    "        j = 0\n",
    "        with open(data_path) as f:\n",
    "            for line in tqdm(f.readlines()):\n",
    "                if line.strip():\n",
    "                    data = json.loads(line)\n",
    "                    id_ = data[\"id\"]\n",
    "                    dialogue = data[\"data\"]\n",
    "                    tags = [i for _ in range(len(dialogue)//2) for i in [\"User\", \"Assistant\"]]\n",
    "                    for i in range(0, len(dialogue), 2):\n",
    "                        tgt_text = dialogue[i+1]\n",
    "                        context = dialogue[:i+1]\n",
    "                        context = zip(tags[:i+1], context)\n",
    "                        context = [\": \".join(item) for item in context]\n",
    "                        example = InputExample(guid=str(j), text_a=\"\", tgt_text=tgt_text, meta={\"context\": context})\n",
    "                        examples.append(example)\n",
    "                        j += 1\n",
    "        return examples\n",
    "\n",
    "\n",
    "    def get_src_tgt_len_ratio(self,):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14125fc1-1320-45da-8288-d221173ade94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conditional Generation with Prefix Tuning.\n",
    "# In this tutorial, we do conditional generation with prefix tuning template.\n",
    "\n",
    "# we use WebNLG as an example, as well. Note that the evaluation of generation result should be done\n",
    "# by using the scripts provided by https://github.com/Yale-LILY/dart/tree/master/evaluation,\n",
    "# Which we do not include in it.\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from openprompt import plms\n",
    "from openprompt.plms import *\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "plms._MODEL_CLASSES[\"gpt2\"]= ModelClass(**{\"config\": GPT2Config, \"tokenizer\": GPT2Tokenizer, \"model\": GPT2LMHeadModel,\n",
    "\"wrapper\": LMTokenizerWrapper})\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import MixedTemplate\n",
    "from transformers import AdamW\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt import PromptForGeneration\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from torchmetrics import MeanMetric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_metrics(metrics, split, prefix=\"\"):\n",
    "    log = f\"[{split}]\" + prefix\n",
    "    log += \" \".join([f\"{key}: {value:.4f}\" for key, value in metrics.items()])\n",
    "\n",
    "    return log\n",
    "\n",
    "def evaluate(args, model, val_dataloader, accelerator):\n",
    "    model.eval()\n",
    "    val_loss = MeanMetric().to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(\n",
    "            tqdm(val_dataloader),\n",
    "        ):\n",
    "                \n",
    "            loss = model(batch[\"input_ids\"])\n",
    "\n",
    "            loss_values = accelerator.gather_for_metrics({\"loss\": loss.detach()})\n",
    "\n",
    "            val_loss.update(loss_values[\"loss\"])\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train(args, accelerator):\n",
    "    set_seed(0)\n",
    "    accelerator.print(f\"Using {accelerator.num_processes} GPUs\")\n",
    "\n",
    "    plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
    "\n",
    "    mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer).from_file(\"./template.txt\")\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processor = UltraChatProcessor()\n",
    "        dataset = processor.get_examples(args.data_file)\n",
    "\n",
    "        train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "    # wrapped_example = mytemplate.wrap_one_example(dataset[1])\n",
    "    # print(wrapped_example)\n",
    "\n",
    "    train_dataloader = PromptDataLoader(dataset=train_dataset, template=mytemplate, tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=1024, decoder_max_length=1024,\n",
    "        batch_size=2,shuffle=True, teacher_forcing=True, predict_eos_token=True, # be sure to pass predict_eos_token=True if your template doesn't contain one, or you model may fail to stop generation.\n",
    "        truncate_method=\"head\").dataloader\n",
    "\n",
    "    val_dataloader = PromptDataLoader(dataset=val_dataset, template=mytemplate, tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=1024, decoder_max_length=1024,\n",
    "        batch_size=5,shuffle=False, teacher_forcing=True, predict_eos_token=True, # be sure to pass predict_eos_token=True if your template doesn't contain one, or you model may fail to stop generation.\n",
    "        truncate_method=\"head\").dataloader\n",
    "\n",
    "\n",
    "    # load the pipeline model PromptForGeneration.\n",
    "    prompt_model = PromptForGeneration(plm=plm, template=mytemplate, tokenizer=tokenizer)\n",
    "\n",
    "    device = accelerator.device\n",
    "    prompt_model.to(device)\n",
    "\n",
    "\n",
    "    optimizer = AdamW([p for p in prompt_model.parameters()if p.requires_grad], lr=args.lr, eps=1e-8)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=200, num_training_steps=len(train_dataloader)*args.epochs)\n",
    "\n",
    "    prompt_model, optimizer, train_dataloader, val_dataloader, scheduler = accelerator.prepare(prompt_model, optimizer, train_dataloader, val_dataloader, scheduler)\n",
    "\n",
    "    accelerator.register_for_checkpointing(scheduler)\n",
    "\n",
    "    train_loss = MeanMetric().to(prompt_model.device)\n",
    "\n",
    "    # training and generation.\n",
    "    global_step = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        for step, inputs in tqdm(enumerate(train_dataloader)):\n",
    "            prompt_model.train()\n",
    "            loss = prompt_model(inputs[\"input_ids\"])\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_values = accelerator.gather_for_metrics({\"loss\": loss.detach()})\n",
    "            train_loss.update(loss_values[\"loss\"])\n",
    "            global_step +=1\n",
    "\n",
    "            \n",
    "            if global_step %50 ==0:\n",
    "                accelerator.save_state(f\"ultrachat_{args.model}/step_{global_step}\")\n",
    "\n",
    "                val_loss = evaluate(args, prompt_model, val_dataloader, accelerator)\n",
    "\n",
    "                log_train = {\n",
    "                        \"train_loss\": train_loss.compute()\n",
    "                    }\n",
    "                log_val = {\n",
    "                    \"val_loss\": val_loss.compute()\n",
    "                }\n",
    "\n",
    "                accelerator.print(f\"Current LR: {scheduler.get_last_lr()[0]}\")\n",
    "                accelerator.print(format_metrics(log_train, \"train\", f\" step {global_step} \"))\n",
    "                accelerator.print(format_metrics(log_val, \"val\", f\" step {global_step} \"))\n",
    "\n",
    "                train_loss.reset()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        torch.save(accelerator.get_state_dict(prompt_model), f\"ultrachat_{args.model}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b296d8-3c35-4271-943e-77e1c7dc62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "tokenizing: 2it [00:00, 167.22it/s]\n",
      "tokenizing: 1it [00:00, 143.32it/s]\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "1it [00:00,  1.64it/s]\n",
      "1it [00:00,  1.80it/s]\n",
      "1it [00:00,  1.77it/s]\n",
      "1it [00:00,  1.81it/s]\n",
      "1it [00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"\")\n",
    "parser.add_argument(\"--lr\", type=float, default=5e-5)\n",
    "parser.add_argument(\"--model\", type=str, default='gpt2')\n",
    "parser.add_argument(\"--model_name_or_path\", default='openai-community/gpt2')\n",
    "parser.add_argument(\"--epochs\", default=5, type=int)\n",
    "parser.add_argument(\"--data_file\", default=\"./data.json\", type=str)\n",
    "args = parser.parse_args(args=[])\n",
    "# print(args)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "train(args, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b279776-7eae-4497-891f-2a08c7da4bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
